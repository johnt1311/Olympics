{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('mlenv': conda)"
  },
  "interpreter": {
   "hash": "62479c27c4a62dbeee051a6978aa30762f6d540183c07f2f567557e7f85cb4c0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 91.0.4472\n",
      "Get LATEST driver version for 91.0.4472\n",
      "Driver [C:\\Users\\tgrah\\.wdm\\drivers\\chromedriver\\win32\\91.0.4472.101\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Set up Splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to website to scrape\n",
    "url = \"https://www.olympedia.org/editions\"\n",
    "browser.visit(url)\n",
    "\n",
    "# parse HTML\n",
    "html = browser.html\n",
    "html_soup = soup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read html with pandas (bs4 and parser working on backend)\n",
    "test_df = pd.read_html(url) # returs list of all tables on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total tables: 8\n"
     ]
    }
   ],
   "source": [
    "print(f'Total tables: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: code below could also be used to pull Winter from the winter table \n",
    "# look for tables with Tokyo\n",
    "table_tokyo = pd.read_html(url, match=\"Tokyo\", converters={\"City\":str})\n",
    "# len(table_tokyo) # returns 1 so that is the table that we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     #  Year       City  Country   Opened    Closed              Competition  \\\n",
       "0    I  1896     Athina      NaN  6 April  15 April           6 â 13 April   \n",
       "1   II  1900      Paris      NaN      NaN       NaN    14 May â 28 October   \n",
       "2  III  1904  St. Louis      NaN   14 May       NaN   1 July â 23 November   \n",
       "3   IV  1908     London      NaN  13 July       NaN  27 April â 31 October   \n",
       "4    V  1912  Stockholm      NaN   6 July   27 July        5 May â 27 July   \n",
       "\n",
       "  Unnamed: 7  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>#</th>\n      <th>Year</th>\n      <th>City</th>\n      <th>Country</th>\n      <th>Opened</th>\n      <th>Closed</th>\n      <th>Competition</th>\n      <th>Unnamed: 7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I</td>\n      <td>1896</td>\n      <td>Athina</td>\n      <td>NaN</td>\n      <td>6 April</td>\n      <td>15 April</td>\n      <td>6 â 13 April</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>II</td>\n      <td>1900</td>\n      <td>Paris</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>14 May â 28 October</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>III</td>\n      <td>1904</td>\n      <td>St. Louis</td>\n      <td>NaN</td>\n      <td>14 May</td>\n      <td>NaN</td>\n      <td>1 July â 23 November</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>IV</td>\n      <td>1908</td>\n      <td>London</td>\n      <td>NaN</td>\n      <td>13 July</td>\n      <td>NaN</td>\n      <td>27 April â 31 October</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>V</td>\n      <td>1912</td>\n      <td>Stockholm</td>\n      <td>NaN</td>\n      <td>6 July</td>\n      <td>27 July</td>\n      <td>5 May â 27 July</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# read table as DF\n",
    "sum_olympic_org = table_tokyo[0]\n",
    "sum_olympic_org.head()\n",
    "\n",
    "# NOTE: country coming in as NaN because it is a picture of a flag. Scrape country codes from the img tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to get the country codes\n",
    "# get the image tag from the correct column in the first table\n",
    "# set table reference\n",
    "table_to_scrape = html_soup.find_all('table')[0]\n",
    "\n",
    "# for td in html_soup.find_all('td'):\n",
    "#     if td.img:\n",
    "#         print(td.img['src'])\n",
    "\n",
    "# get table rows\n",
    "table_body = table_to_scrape.find_all('tr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of rows in the table\n",
    "body_rows = table_body[0:]\n",
    "# len(body_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through rows and get the image source tags\n",
    "\n",
    "\n",
    "# define empty list to hold the src tags\n",
    "flag_images = []\n",
    "\n",
    "# set row number to pass to table_body[]\n",
    "for row_num in range(len(body_rows)):\n",
    "\n",
    "    # loop through each row in the table body and find all rows\n",
    "    for row in table_body[row_num].find_all('td'):\n",
    "         \n",
    "        # check to see if the row has an img tag \n",
    "        if row.img:\n",
    "            # append the img src tag to a list\n",
    "            flag_images.append(row.img['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to get the country code from the img src\n",
    "import re\n",
    "country_codes = []\n",
    "# loop through list and pull 3 alphanumeric before '.'\n",
    "for src in flag_images:\n",
    "    code = re.findall(r\"(\\w{3})\\.\", src) # maybe not the most efficient way but all the strings were the same so took a shot at it\n",
    "    # append to list\n",
    "    country_codes.append(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex loop returned a list of lists so extract into single list\n",
    "country_codes = [x[0] for x in country_codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Year       City  Country\n",
       "0  1896     Athina      NaN\n",
       "1  1900      Paris      NaN\n",
       "2  1904  St. Louis      NaN\n",
       "3  1908     London      NaN\n",
       "4  1912  Stockholm      NaN"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>City</th>\n      <th>Country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1896</td>\n      <td>Athina</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1900</td>\n      <td>Paris</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1904</td>\n      <td>St. Louis</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1908</td>\n      <td>London</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1912</td>\n      <td>Stockholm</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# get rid of columns we don't need\n",
    "sum_olympic_cln = sum_olympic_org[['Year', 'City', 'Country']].copy()\n",
    "sum_olympic_cln.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Year       City Country\n",
       "0  1896     Athina     GRE\n",
       "1  1900      Paris     FRA\n",
       "2  1904  St. Louis     USA\n",
       "3  1908     London     GBR\n",
       "4  1912  Stockholm     SWE"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>City</th>\n      <th>Country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1896</td>\n      <td>Athina</td>\n      <td>GRE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1900</td>\n      <td>Paris</td>\n      <td>FRA</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1904</td>\n      <td>St. Louis</td>\n      <td>USA</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1908</td>\n      <td>London</td>\n      <td>GBR</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1912</td>\n      <td>Stockholm</td>\n      <td>SWE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# replace the NaNs in the Country column with country_codes list\n",
    "sum_olympic_cln['Country'] = country_codes\n",
    "sum_olympic_cln.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 34 entries, 0 to 33\nData columns (total 3 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   Year     34 non-null     int64 \n 1   City     34 non-null     object\n 2   Country  34 non-null     object\ndtypes: int64(1), object(2)\nmemory usage: 944.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# check DF\n",
    "sum_olympic_cln.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of everything before 1964 and after 2016\n",
    "analysis_years = sum_olympic_cln.copy()\n",
    "analysis_years.drop(analysis_years[(analysis_years['Year'] < 1964) | (analysis_years['Year'] > 2016)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_years.rename(columns={\"City\": \"Host_City\", \"Country\":\"Host_Country\"}, inplace=True)\n",
    "# analysis_years.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send years with host cities and countries to csv\n",
    "# analysis_years.to_csv(\"host_cities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the dataframes - doing this after becuase I didn't want to re-run the scrape.\n",
    "\n",
    "# load host cities as DF\n",
    "# host_cities_df = pd.read_csv(\"host_cities.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load medals scrape csv as DF\n",
    "# all_country_medals_df = pd.read_csv(\"complete_year_datascrape_07182021.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the dataframes on year\n",
    "# combined_data = all_country_medals_df.merge(right=host_cities_df, how=\"left\", on=\"Year\")\n",
    "# combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send merged dataframe to .csv\n",
    "# combined_data.to_csv(\"medal_data_by_year.csv\")"
   ]
  },
  {
   "source": [
    "# Moving to individual year medals pages\n",
    "1 - navigate to correct page\n",
    "\n",
    "2 - locate medal table\n",
    "\n",
    "3 - load medal table as DF with index as the year\n",
    "\n",
    "4 - merge with the summer olympic table - "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get years into a list so you can use them for href and click\n",
    "years_list = analysis_years['Year'].tolist()\n",
    "\n",
    "# years are int - need them to be string to pass to scraping loop so convert\n",
    "years_list = [str(x) for x in years_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xpath we know the table row to start is 18 \n",
    "# browser.find_by_xpath('/html/body/div[2]/table[1]/tbody/tr[18]/td[2]/a'). click() # this also works but is less understandable\n",
    "\n",
    "# NOTE: according to this link https://stackoverflow.com/questions/13784192/creating-an-empty-pandas-dataframe-then-filling-it it is not a good idea to append dataframes in a loop. Working on scraping to lists or dictionaries and then turning into dataframe but this loop should work despite it being memory intensive if we need to use it. \n",
    "\n",
    "# create the first dataframe - 1964\n",
    "# complete_medals_table = Data - was going to create a blank dataframe to append to but apparently that's not a good idea\n",
    "\n",
    "# test list = successful for 2 years\n",
    "test_years = ['1964', '1968']\n",
    "\n",
    "# set iterator\n",
    "# counter = 0\n",
    "\n",
    "# df dictionary - testing\n",
    "# https://stackoverflow.com/questions/30233982/merge-dataframes-in-a-dictionary\n",
    "\n",
    "all_years = {}\n",
    "\n",
    "# start for loop to get all of the year medal tables\n",
    "for year in years_list: # change back to years_list when testing complete\n",
    "\n",
    "    browser.find_by_text(year).click() # this works - loop step 1\n",
    "\n",
    "    # get page url - loop step 2\n",
    "    page_url = browser.url\n",
    "\n",
    "    # reset soup\n",
    "    # parse HTML\n",
    "    html = browser.html\n",
    "    html_soup = soup(html, 'html.parser')\n",
    "\n",
    "    # get all tables on the page as DFs \n",
    "    page_dfs = pd.read_html(page_url)\n",
    "    # print(f'Total tables: {len(page_dfs)}') # check number of tables on page - use for debugging\n",
    "\n",
    "    # get the medal table and load to DF\n",
    "    # medals_table = pd.read_html(page_url, match=\"NOC\")[0] # loop step 3\n",
    "    all_years[year] = pd.read_html(page_url, match=\"NOC\")[0] # loop step 3\n",
    "\n",
    "    # add column 'year' with the year of the games - loop step 4\n",
    "    # medals_table['Year'] = int(year)\n",
    "    all_years[year]['Year'] = int(year)\n",
    "\n",
    "    # rename the columns - loop step 5\n",
    "    # medals_table.rename(columns={'NOC': 'Country', 'NOC.1': 'Country Code'}, inplace=True)\n",
    "    all_years[year].rename(columns={'NOC': 'Country', 'NOC.1': 'Country Code'}, inplace=True)\n",
    "\n",
    "    # NOTE: may get rid of this in favor of adding to dictionary and then appending.\n",
    "    # if this is the first year then just save as DF, otherwise append to the DF\n",
    "    # if counter != 0:\n",
    "        # medals_table = medals_table.append(medals_table, ignore_index=True)\n",
    "\n",
    "    # counter += 1\n",
    "\n",
    "    # go back to the main page\n",
    "    browser.back()\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "# check that we pulled all years should = 14 - looks good so far\n",
    "len(all_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(865, 7)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Country Country Code  Gold  Silver  Bronze  Total  Year\n",
       "0  United States          USA    36      26      28     90  1964\n",
       "1   Soviet Union          URS    30      31      35     96  1964\n",
       "2          Japan          JPN    16       5       8     29  1964\n",
       "3        Germany          GER    10      22      18     50  1964\n",
       "4          Italy          ITA    10      10       7     27  1964"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country</th>\n      <th>Country Code</th>\n      <th>Gold</th>\n      <th>Silver</th>\n      <th>Bronze</th>\n      <th>Total</th>\n      <th>Year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>United States</td>\n      <td>USA</td>\n      <td>36</td>\n      <td>26</td>\n      <td>28</td>\n      <td>90</td>\n      <td>1964</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Soviet Union</td>\n      <td>URS</td>\n      <td>30</td>\n      <td>31</td>\n      <td>35</td>\n      <td>96</td>\n      <td>1964</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Japan</td>\n      <td>JPN</td>\n      <td>16</td>\n      <td>5</td>\n      <td>8</td>\n      <td>29</td>\n      <td>1964</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Germany</td>\n      <td>GER</td>\n      <td>10</td>\n      <td>22</td>\n      <td>18</td>\n      <td>50</td>\n      <td>1964</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Italy</td>\n      <td>ITA</td>\n      <td>10</td>\n      <td>10</td>\n      <td>7</td>\n      <td>27</td>\n      <td>1964</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "complete_summer_data = pd.concat(all_years.values(), ignore_index=True)\n",
    "print(complete_summer_data.shape)\n",
    "complete_summer_data.Year.unique().tolist()\n",
    "complete_summer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to a csv to double check the data\n",
    "complete_summer_data.to_csv('complete_country_medals_by_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Year    Host_City Host_Country\n",
       "0  1964        Tokyo          JPN\n",
       "1  1968  Mexico City          MEX\n",
       "2  1972       Munich          GER\n",
       "3  1976     Montreal          CAN\n",
       "4  1980       Moskow          RUS"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>Host_City</th>\n      <th>Host_Country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1964</td>\n      <td>Tokyo</td>\n      <td>JPN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1968</td>\n      <td>Mexico City</td>\n      <td>MEX</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1972</td>\n      <td>Munich</td>\n      <td>GER</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1976</td>\n      <td>Montreal</td>\n      <td>CAN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1980</td>\n      <td>Moskow</td>\n      <td>RUS</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "# merge dataframes on year\n",
    "analysis_years_df = pd.read_csv(\"host_cities.csv\")\n",
    "analysis_years_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(865, 9)\n"
     ]
    }
   ],
   "source": [
    "combined_data = pd.merge(complete_summer_data, analysis_years_df, how=\"left\", on=[\"Year\"])\n",
    "combined_data.head()\n",
    "print(combined_data.shape)"
   ]
  },
  {
   "source": [
    "# Below is working on potentially parsing each table into a list and then making the dataframe from the list of lists. \n",
    "\n",
    "### NOTE: \n",
    "\n",
    "It looks like the loop above is going to work and I doubt we are going to get graded on the time the code takes to process but left the link and maybe this challenge in here if we want to do it the 'most effective' way.\n",
    "\n",
    "Maybe this would be an issue if we had more data but it didn't seem like an issue for the medals by year. \n",
    "\n",
    "link with good info on why not to append dataframes but not related to scraping \n",
    "- https://stackoverflow.com/questions/13784192/creating-an-empty-pandas-dataframe-then-filling-it \n",
    "\n",
    "## Ideas: Maybe pull tables with pd.read_html still and then send each column to a list with tolist(). Append those lists to a master list or dictionary, and then make a combined df with the data?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# started some testing on scraping to lists\n",
    "\n",
    "# /html/body/div[2]/table[5]/thead/tr/th[1]\n",
    "url = \"https://www.olympedia.org/editions/16\"\n",
    "\n",
    "html = browser.html\n",
    "html_soup = soup(html, 'html.parser')\n",
    "\n",
    "# set table reference\n",
    "year_results_table = html_soup.find_all('table')[5]\n",
    "table_headers = []\n",
    "for year_results_table\n",
    "# get the table headers\n",
    "# set the headers as dictionary keys\n",
    "# append column values to the key:values\n",
    "# go to the next page and repeat\n",
    "# turn the dictionary into a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}